{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import paddlenlp\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到数据集字典\n",
    "def open_func(file_path):\n",
    "    return [line.strip() for line in open(file_path, 'r', encoding='utf8').readlines()[1:] if len(line.strip().split('\\t')) >= 2]\n",
    "\n",
    "data_dict = {'SBIC': {'test': open_func('SBIC/test.tsv'),\n",
    "                              'dev': open_func('SBIC/dev.tsv'),\n",
    "                              'train': open_func('SBIC/train.tsv')},\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "from paddle.io import Dataset, DataLoader\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "import numpy as np\n",
    "label_list = [0, 1]\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512, for_test=False):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "        self._for_test = for_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        samples = self._data[idx].split('\\t')\n",
    "        label = samples[-2]\n",
    "        text = samples[-1]\n",
    "        label = int(label)\n",
    "        text = self._tokenizer.encode(text, max_seq_len=self._max_len)['input_ids']\n",
    "        if self._for_test:\n",
    "            return np.array(text, dtype='int64')\n",
    "        else:\n",
    "            return np.array(text, dtype='int64'), np.array(label, dtype='int64')\n",
    "\n",
    "def batchify_fn(for_test=False):\n",
    "    if for_test:\n",
    "        return lambda samples, fn=Pad(axis=0, pad_val=tokenizer.pad_token_id): np.row_stack([data for data in fn(samples)])\n",
    "    else:\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\n",
    "                                        Stack()): [data for data in fn(samples)]\n",
    "\n",
    "\n",
    "def get_data_loader(data, tokenizer, batch_size=32, max_len=512, for_test=False):\n",
    "    dataset = MyDataset(data, tokenizer, max_len, for_test)\n",
    "    shuffle = True if not for_test else False\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=batchify_fn(for_test), shuffle=shuffle)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.static import InputSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型和分词\n",
    "model = SkepForSequenceClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=2)\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')\n",
    "\n",
    "# 参数设置\n",
    "data_name = 'SBIC'  # 更改此选项改变数据集\n",
    "\n",
    "## 训练相关\n",
    "epochs = 5\n",
    "learning_rate = 2e-5\n",
    "batch_size = 16\n",
    "max_len = 256\n",
    "\n",
    "## 数据相关\n",
    "train_dataloader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_len, for_test=False)\n",
    "if data_name == 'SBIC':\n",
    "    dev_dataloader = get_data_loader(data_dict[data_name]['dev'], tokenizer, batch_size, max_len, for_test=False)\n",
    "else:\n",
    "    dev_dataloader = None\n",
    "\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\n",
    "label = InputSpec((-1, 2), dtype='int64', name='label')\n",
    "model = paddle.Model(model, [input], [label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型准备\n",
    "\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\n",
    "model.prepare(optimizer, loss=paddle.nn.CrossEntropyLoss(), metrics=[paddle.metric.Accuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2021-06-18 15:45:38,266] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
    "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
    "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
    "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
    "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
    "[2021-06-18 15:45:49,343] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "model.fit(train_dataloader, dev_dataloader, batch_size, epochs, eval_freq=5, save_freq=1, save_dir='./checkpoints', log_freq=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
    "Epoch 1/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
    "  return (isinstance(seq, collections.Sequence) and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 200/600 - loss: 0.2172 - acc: 0.8881 - 647ms/step\n",
    "step 400/600 - loss: 0.1809 - acc: 0.9062 - 650ms/step\n",
    "step 600/600 - loss: 0.1273 - acc: 0.9123 - 648ms/step\n",
    "save checkpoint at /home/aistudio/checkpoints/0\n",
    "Eval begin...\n",
    "step 75/75 - loss: 0.0813 - acc: 0.9450 - 229ms/step\n",
    "Eval samples: 1200\n",
    "Epoch 2/5\n",
    "step 200/600 - loss: 0.0296 - acc: 0.9706 - 647ms/step\n",
    "step 400/600 - loss: 0.0918 - acc: 0.9675 - 649ms/step\n",
    "step 600/600 - loss: 0.0284 - acc: 0.9675 - 649ms/step\n",
    "save checkpoint at /home/aistudio/checkpoints/1\n",
    "Epoch 3/5\n",
    "step 200/600 - loss: 0.0048 - acc: 0.9838 - 647ms/step\n",
    "step 400/600 - loss: 0.2752 - acc: 0.9827 - 644ms/step\n",
    "step 600/600 - loss: 0.0124 - acc: 0.9834 - 645ms/step\n",
    "save checkpoint at /home/aistudio/checkpoints/2\n",
    "Epoch 4/5\n",
    "step 200/600 - loss: 0.0023 - acc: 0.9866 - 652ms/step\n",
    "step 400/600 - loss: 0.0060 - acc: 0.9881 - 653ms/step\n",
    "step 600/600 - loss: 0.0047 - acc: 0.9866 - 649ms/step\n",
    "save checkpoint at /home/aistudio/checkpoints/3\n",
    "Epoch 5/5\n",
    "step 200/600 - loss: 7.8751e-04 - acc: 0.9884 - 650ms/step\n",
    "step 400/600 - loss: 0.0011 - acc: 0.9905 - 648ms/step\n",
    "step 600/600 - loss: 0.0012 - acc: 0.9908 - 649ms/step\n",
    "save checkpoint at /home/aistudio/checkpoints/4\n",
    "save checkpoint at /home/aistudio/checkpoints/final"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
